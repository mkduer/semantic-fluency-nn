# t-SNE Visualizations for Word Embeddings

1. [Introduction](#introduction)
2. [Setup](#setup)
3. [Corpus Preparation](#corpus)
4. [Generate Plots Titled with Hyperparameter Details](#generate_plots)
5. [Examples of tSNE Visualizations](#examples)
   * [2-Dimensional t-SNE Plot](#2D)
   * [3-Dimensional t-SNE Plot](#3D)
   * [t-SNE Cluster Plot](#cluster)


## 1 Introduction  <a name="introduction"></a>
This t-SNE module allows for 2-dimensional and 3-dimensional visualizations of Word2Vec embeddings generated from a text corpus. In addition, subsets of similar words can be clustered and visualized based on specific corpus keywords that are passed in.

Note that t-SNE, t-distributed Stochastic Neighbor Embedding, is not meant to give perfect mathematical representations of data. Rather, it is a visualization tool for reducing high-dimensional data into smaller dimensions to allow for any type of comprehensible visualization. More about this module can be found here: [sklearn.manifold.TSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html).

That being said, t-SNE can be a very fun way to cut through dense data and allow for human-readable representations of high-dimensional data. The module does tend to require an extensive amount of hyperparameter fine-tuning. Slight tweaks in the hyperparameters can result in very different visualizations, so knowing what you expect from your data and what you're trying to relay is quite important prior to creating these visualizations. Examples are provided below.

Note: the Shakespeare corpus was generated from available content in Project Gutenberg ([Shakespeare Corpus Source](http://www.gutenberg.org/files/100/100-h/100-h.htm)). 

### 2 Setup  <a name="setup"></a>
Import necessary modules.

```python
import os
os.chdir('..')
```

```python
from sklearn.manifold import TSNE  # imports t-SNE to visualize embeddings
import matplotlib.pyplot as plt  # allows for plotting
from mpl_toolkits.mplot3d import Axes3D  # allows for 3D plotting
import matplotlib.cm as cm  # allows for coloring data points
import numpy as np  # imports numpy for matrix operations

from os.path import exists  # check that path exists
from os import mkdir  # directory operations
from shutil import rmtree  # remove specific directory contents
import constants as C  # constants
```


### 3 Corpus Preparation  <a name="corpus"></a>
Clean the textual corpus (tokenize and lematize) and generate a Word2Vec model from the resulting corpus.

```python
shakespeare = ""
corpus_name = 'Shakespeare Corpus: '
with open('shakespeare.txt', 'r', encoding="utf-8") as myfile:
    shakespeare=myfile.read().replace('\n', '')
        
shakespeare_sentences = sent_tokenize(shakespeare)
tokenizer = RegexpTokenizer(r'\w+')
lemmer = WordNetLemmatizer()
sentence_2d = [tokenizer.tokenize(sent) for sent in shakespeare_sentences]
sentence_2d = [[lemmer.lemmatize(word.lower()) for word in sent] for sent in sentence_2d]

model = Word2Vec(sentence_2d, size=120, window=5, min_count=2, workers=8, sg=1)
model.train(sentence_2d, total_examples=len(sentence_2d),
            epochs=1, start_alpha=0.1, compute_loss=True)
```


### 4 Generate Plots Titled with Hyperparameter Details  <a name="generate_plots"></a>
Note that the hyperparameters have been set as constants in the ```constants.py``` file.

```python
plot = TsnePlot()

# Process the vectors generated by word2vec
words, tokens = plot.process_vectors(model.wv)

# Generate 2D Embeddings Plot
reduced_model_2D = plot.reduce_model(tokens, C.C_PERPLEXITY, C.C_COMPONENTS, C.C_ITER, C.C_ETA)
img_title = plot.save_title(C.C_PERPLEXITY, C.C_COMPONENTS, C.C_ITER, C.C_ETA)
plot_title = corpus_name + '2-Dimensional Word Embeddings'
plot.visualize_embeddings_2D(reduced_model_2D, words, plot_title, img_title)
    
# Generate 3D Embeddings Plot
reduced_model_3D = plot.reduce_model(tokens, C.DIM3_PERPLEXITY, C.DIM3_COMPONENTS, C.DIM3_ITER, C.DIM3_ETA)
img_title = plot.save_title(C.DIM3_PERPLEXITY, C.DIM3_COMPONENTS, C.DIM3_ITER, C.DIM3_ETA)
plot_title = corpus_name + '3-Dimensional Word Embeddings'
plot.visualize_embeddings_3D(reduced_model_3D, words, plot_title, img_title)

# Create Cluster Plot
words, clusters = plot.similarity_clusters(model.wv, C.CLUSTER_KEYS)
reduced_clusters = plot.reduce_clusters(clusters)
img_title = plot.save_title(C.C_PERPLEXITY, C.C_COMPONENTS, C.C_ITER, '_top ' + str(C.TOP_N))
plot_title = corpus_name + 'Word Embeddings (Top ' + str(C.TOP_N) + ' Most Similar)'
plot.visualize_clusters(reduced_clusters, words, C.CLUSTER_KEYS, plot_title, img_title)
```


### 5 Examples of tSNE Visualizations  <a name="examples"></a>

The 2-dimensional embeddings could be saved in the following format displaying what hyperparemter settings were applied:

```shakespeare_tSNE2D_perplexity5_components2_iter2400_eta450```

This will allow for hyperparmeter fine-tuning, which can result in drastically different results with t-SNE. The following examples all use the Shakespeare corpus to generate visualizations of the embeddings.

##### 2-Dimensional t-SNE Plot  <a name="2D"></a>

![alt text](https://github.com/mkduer/semantic-fluency-nn/tree/master/docs/example_images/shakespeare_tSNE2D_perplexity5_components2_iter2400_eta450 "2D tSNE plot")

##### 3-Dimensional t-SNE Plot  <a name="3D"></a>

![alt text](https://github.com/mkduer/semantic-fluency-nn/tree/master/docs/example_images/shakespeare_tSNE3D_perplexity10_components3_iter2400_eta400 "3D tSNE plot")

##### t-SNE Cluster Plot  <a name="cluster"></a>

![alt text](https://github.com/mkduer/semantic-fluency-nn/tree/master/docs/example_images/shakespeare_tSNE2D_perplexity5_components2_iter2400_eta450_top100 "tSNE cluster plot")